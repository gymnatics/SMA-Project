plot(history)
# Include your libraries here:
library(ggplot2)
library(MASS) # for parallel coordinates plot
library(dplyr)
library(keras)
library(tidyr)
library(stringr)
library(conflicted) # useful for debugging
# The MASS package has functions for filter() and select(): a conflict with dplyr
conflict_prefer("filter", "dplyr")  # use filter() from dplyr
conflict_prefer("select", "dplyr") # use select() from dplyr
use_condaenv("tf-2.6")
e_s1 <- 10
e_s2 <- seq(0, 20, by = 0.2)
e_s3 <- 10
partial <- ((e_s1 + e_s3) ^ e_s2) / (e_s1 + e_s2 + e_s3) ^ 2
temp_df <- data.frame(e_s2, partial)
# plot the graph
ggplot(temp_df, aes(x = e_s2, y = partial)) + geom_point()
p <- c(1/32,8/32,15/32,3/32,1/32,1/32,1/32,2/32)
entropy <- - (p * log2(p))
print(entropy)
maxentropy <- max(entropy)
print(maxentropy)
dfwords <- as.data.frame(list(Class=c("Rat","Ox","Tiger","Rabbit","Dragon","Snake","Horse","Sheep"),Prob=c(1/32,8/32,15/32,3/32,1/32,1/32,1/32,2/32),Freq=c(3,24,50,9,3,2,3,10)))
# print(dfwords)
# Extract the relevant columns
probs <- dfwords$Prob
labels <- as.matrix(diag(dfwords$Freq))
# Create the probability matrix
p <- matrix(probs, nrow = nrow(labels), ncol = length(probs), byrow = TRUE)
# Compute the cross-entropy loss
cross_entropy <- -mean(apply(labels * log2(p), 1, sum))
print(cross_entropy)
# Classification with Keras
# Read in the data (from https://www.kaggle.com/pablocastilla/classification-with-keras/data)
raw_data <- read.csv("data.csv")
#View(raw_data)
# Drop the first and last columns (they contain no relevant data)
numcolumns <- ncol(raw_data)
bc_data <- raw_data %>% select(-1,-all_of(numcolumns))
# The number of features (X variables) is numcolumns -3
numfeatures <- numcolumns-3
# View(bc_data)
# Now, let's find a way to visualize the data.
# Let's try parallel coordinates from the MASS package
# Example: https://r-charts.com/ranking/parallel-coordinates/
bc_data_plot <- bc_data
# First convert the categorical column to an integer column
bc_data_plot$diagnosis <- factor(bc_data_plot$diagnosis)
bc_data_plot$diagnosis <- as.integer(unclass(bc_data_plot$diagnosis))
head(bc_data_plot)
# The value 1 represents a diagnosis of "benign"; the value 2 represents "malignant".
all_colors <- c("lightgray","blue") # blue will represent malignant diagnosis, lightgray is benign
# Sort so that the malignant cases are on top of the plot (at bottom of dataframe)
bc_plotdata <- bc_data_plot[, c(1:30)] %>% arrange(diagnosis)
bc_colors <- all_colors[factor(bc_plotdata$diagnosis)]
parcoord(bc_plotdata , col = all_colors)
# Sort so that the benign cases are on top of the plot (on bottom of dataframe)
bc_plotdata <- bc_data_plot[, c(1:30)] %>% arrange(desc(diagnosis))
bc_colors <- all_colors[factor(bc_plotdata$diagnosis)]
parcoord(bc_plotdata , col = all_colors)
# Let's return to our original dataset, bc_data
# View(bc_data)
# Add a column of ones
bc_data$ones <- 1
# Use pivot_wider to create a column for each unique value in the diagnosis column
bc_encoded <- bc_data %>% pivot_wider(names_from=diagnosis,values_from=ones)
# Replace the NA's with zeroes
bc_encoded[is.na(bc_encoded)] <- 0
# That's all there is to it.
# View(bc_encoded)
# The last two columns of our encoded data frame are the one hot encoding of the diagnosis column
# The diagnosis column has been removed.
# Convert from data frame to matrix
bcmatrix <- as.matrix(bc_encoded)
# Remove the dimension names
#dimnames(bcmatrix) <- NULL
# The features are the first columns of the matrix (the last two columns are one hot encoding of Y)
features <- bcmatrix[, 1:(ncol(bcmatrix) - 2)]  # select and scale the features
#View(features)
# The y variates are the last two columns of bcmatrix
y_encoded <- bcmatrix[, (ncol(bcmatrix) - 1):ncol(bcmatrix)]
#View(y_encoded)
rowcount <- nrow(features)
set.seed(123)
trainrows <- sort(sample(rowcount, rowcount * 0.7))
trainFeatures <- features[trainrows, ]
testFeatures <- features[-trainrows, ]
trainLabels <- y_encoded[trainrows, ]
testLabels <- y_encoded[-trainrows, ]
model1 <-  keras_model_sequential()
model1 %>%
layer_dense(units = 2, activation = "softmax", input_shape = ncol(trainFeatures))
model1 %>% summary
# compile the model
model1 %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_sgd(),
metrics = c ("accuracy")
)
# fit the model
history = model1 %>% fit(
x = trainFeatures,
y = trainLabels,
epochs = 200
)
plot(history)
# extract the accuracy of Model 1
last_epoch <- max(history$params$epochs)
accuracy <- history$metrics$val_accuracy[last_epoch]
print(str_c("Model 1 accuracy: ", accuracy))
View(history)
# extract the accuracy of Model 1
last_epoch <- max(history$params$epochs)
accuracy <- history$metrics$accuracy[last_epoch]
print(str_c("Model 1 accuracy: ", accuracy))
# extract the accuracy of Model 1
last_epoch <- max(history$params$epochs)
accuracy <- history$metrics$accuracy[last_epoch]
print(str_c("Model 1 accuracy: ", round(accuracy, 3))
# extract the accuracy of Model 1
last_epoch <- max(history$params$epochs)
accuracy <- history$metrics$accuracy[last_epoch]
print(str_c("Model 1 accuracy: ", round(accuracy, 3)))
model2 <-  keras_model_sequential()
model2 %>%
layer_dense(units = 30, activation = "relu", input_shape = ncol(trainFeatures),
bias_regularizer = regularizer_l1(0.01),
activity_regularizer = regularizer_l1(0.01)) %>%
layer_dense(units = 2, activation = "softmax",
bias_regularizer = regularizer_l1(0.01),
activity_regularizer = regularizer_l1(0.01))
model2%>% summary
?layer_dense
# Include your libraries here:
library(ggplot2)
library(MASS) # for parallel coordinates plot
library(dplyr)
library(keras)
library(tidyr)
library(stringr)
library(conflicted) # useful for debugging
# The MASS package has functions for filter() and select(): a conflict with dplyr
conflict_prefer("filter", "dplyr")  # use filter() from dplyr
conflict_prefer("select", "dplyr") # use select() from dplyr
use_condaenv("tf-2.6")
e_s1 <- 10
e_s2 <- seq(0, 20, by = 0.2)
e_s3 <- 10
partial <- ((e_s1 + e_s3) ^ e_s2) / (e_s1 + e_s2 + e_s3) ^ 2
temp_df <- data.frame(e_s2, partial)
# plot the graph
ggplot(temp_df, aes(x = e_s2, y = partial)) + geom_point()
p <- c(1/32,8/32,15/32,3/32,1/32,1/32,1/32,2/32)
entropy <- - (p * log2(p))
print(entropy)
maxentropy <- max(entropy)
print(maxentropy)
dfwords <- as.data.frame(list(Class=c("Rat","Ox","Tiger","Rabbit","Dragon","Snake","Horse","Sheep"),Prob=c(1/32,8/32,15/32,3/32,1/32,1/32,1/32,2/32),Freq=c(3,24,50,9,3,2,3,10)))
# print(dfwords)
# Extract the relevant columns
probs <- dfwords$Prob
labels <- as.matrix(diag(dfwords$Freq))
# Create the probability matrix
p <- matrix(probs, nrow = nrow(labels), ncol = length(probs), byrow = TRUE)
# Compute the cross-entropy loss
cross_entropy <- -mean(apply(labels * log2(p), 1, sum))
print(cross_entropy)
# Classification with Keras
# Read in the data (from https://www.kaggle.com/pablocastilla/classification-with-keras/data)
raw_data <- read.csv("data.csv")
#View(raw_data)
# Drop the first and last columns (they contain no relevant data)
numcolumns <- ncol(raw_data)
bc_data <- raw_data %>% select(-1,-all_of(numcolumns))
# The number of features (X variables) is numcolumns -3
numfeatures <- numcolumns-3
# View(bc_data)
# Now, let's find a way to visualize the data.
# Let's try parallel coordinates from the MASS package
# Example: https://r-charts.com/ranking/parallel-coordinates/
bc_data_plot <- bc_data
# First convert the categorical column to an integer column
bc_data_plot$diagnosis <- factor(bc_data_plot$diagnosis)
bc_data_plot$diagnosis <- as.integer(unclass(bc_data_plot$diagnosis))
head(bc_data_plot)
# The value 1 represents a diagnosis of "benign"; the value 2 represents "malignant".
all_colors <- c("lightgray","blue") # blue will represent malignant diagnosis, lightgray is benign
# Sort so that the malignant cases are on top of the plot (at bottom of dataframe)
bc_plotdata <- bc_data_plot[, c(1:30)] %>% arrange(diagnosis)
bc_colors <- all_colors[factor(bc_plotdata$diagnosis)]
parcoord(bc_plotdata , col = all_colors)
# Sort so that the benign cases are on top of the plot (on bottom of dataframe)
bc_plotdata <- bc_data_plot[, c(1:30)] %>% arrange(desc(diagnosis))
bc_colors <- all_colors[factor(bc_plotdata$diagnosis)]
parcoord(bc_plotdata , col = all_colors)
# Let's return to our original dataset, bc_data
# View(bc_data)
# Add a column of ones
bc_data$ones <- 1
# Use pivot_wider to create a column for each unique value in the diagnosis column
bc_encoded <- bc_data %>% pivot_wider(names_from=diagnosis,values_from=ones)
# Replace the NA's with zeroes
bc_encoded[is.na(bc_encoded)] <- 0
# That's all there is to it.
# View(bc_encoded)
# The last two columns of our encoded data frame are the one hot encoding of the diagnosis column
# The diagnosis column has been removed.
# Convert from data frame to matrix
bcmatrix <- as.matrix(bc_encoded)
# Remove the dimension names
#dimnames(bcmatrix) <- NULL
# The features are the first columns of the matrix (the last two columns are one hot encoding of Y)
features <- bcmatrix[, 1:(ncol(bcmatrix) - 2)]  # select and scale the features
#View(features)
# The y variates are the last two columns of bcmatrix
y_encoded <- bcmatrix[, (ncol(bcmatrix) - 1):ncol(bcmatrix)]
#View(y_encoded)
rowcount <- nrow(features)
set.seed(123)
trainrows <- sort(sample(rowcount, rowcount * 0.7))
trainFeatures <- features[trainrows, ]
testFeatures <- features[-trainrows, ]
trainLabels <- y_encoded[trainrows, ]
testLabels <- y_encoded[-trainrows, ]
model1 <-  keras_model_sequential()
model1 %>%
layer_dense(units = 2, activation = "softmax", input_shape = ncol(trainFeatures))
model1 %>% summary
# compile the model
model1 %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_sgd(),
metrics = c ("accuracy")
)
# fit the model
history = model1 %>% fit(
x = trainFeatures,
y = trainLabels,
epochs = 200,
batch_size = 20,
validation_split = 0.2
)
plot(history)
# extract the accuracy of Model 1
last_epoch <- max(history$params$epochs)
accuracy <- history$metrics$accuracy[last_epoch]
print(str_c("Model 1 accuracy: ", round(accuracy, 3)))
model2 <-  keras_model_sequential()
model2 %>%
layer_dense(units = 30, activation = "relu", input_shape = ncol(trainFeatures),
bias_regularizer = regularizer_l1(0.01),
activity_regularizer = regularizer_l1(0.01)) %>%
layer_dense(units = 2, activation = "softmax",
bias_regularizer = regularizer_l1(0.01),
activity_regularizer = regularizer_l1(0.01))
model2%>% summary
# compile the model
model2 %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_sgd(),
metrics = c ("accuracy")
)
# fit the model
history = model2 %>% fit(
x = trainFeatures,
y = trainLabels,
epochs = 200,
batch_size = 20,
validation_split = 0.2
)
plot(history)
# extract the accuracy of Model 2
last_epoch <- max(history$params$epochs)
accuracy <- history$metrics$accuracy[last_epoch]
print(str_c("Model 2 accuracy:", round(accuracy, 3)))
# extract the accuracy of Model 2
last_epoch <- max(history$params$epochs)
accuracy <- history$metrics$accuracy[last_epoch]
print(str_c("Model 2 accuracy: ", round(accuracy, 3)))
model2 <-  keras_model_sequential()
model2 %>%
layer_dense(units = 30, activation = "relu", input_shape = ncol(trainFeatures),
bias_regularizer = regularizer_l1(0.01),
activity_regularizer = regularizer_l1(0.01)) %>%
layer_dense(units = 2, activation = "softmax")
model2%>% summary
# compile the model
model2 %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_sgd(),
metrics = c ("accuracy")
)
# fit the model
history = model2 %>% fit(
x = trainFeatures,
y = trainLabels,
epochs = 200,
batch_size = 20,
validation_split = 0.2
)
plot(history)
# extract the accuracy of Model 2
last_epoch <- max(history$params$epochs)
accuracy <- history$metrics$accuracy[last_epoch]
print(str_c("Model 2 accuracy: ", round(accuracy, 3)))
model3 <-  keras_model_sequential()
model3 %>%
layer_dense(units = 30, activation = "relu", input_shape = ncol(trainFeatures),
bias_regularizer = regularizer_l1(0.1),
activity_regularizer = regularizer_l1(0.05)) %>%
layer_dense(units = 2, activation = "softmax")
model3%>% summary
# compile the model
model3 %>% compile(
loss = "categorical_crossentropy",
optimizer = optimizer_sgd(),
metrics = c ("accuracy")
)
# fit the model
history = model3 %>% fit(
x = trainFeatures,
y = trainLabels,
epochs = 200,
batch_size = 20,
validation_split = 0.2
)
plot(history)
# extract the accuracy of Model 3
last_epoch <- max(history$params$epochs)
accuracy <- history$metrics$accuracy[last_epoch]
print(str_c("Model 3 accuracy: ", round(accuracy, 3)))
gc()
# Include your libraries here:
library(ggplot2)
library(MASS) # for parallel coordinates plot
library(dplyr)
library(keras)
library(tidyr)
library(stringr)
library(conflicted) # useful for debugging
# The MASS package has functions for filter() and select(): a conflict with dplyr
conflict_prefer("filter", "dplyr")  # use filter() from dplyr
conflict_prefer("select", "dplyr") # use select() from dplyr
use_condaenv("tf-2.6")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(stringr)
library(geometry) #dot() function for dotproduct
library(keras)
library(deepviz)  # not essential but nice to show structure
use_condaenv("tf-2.6")
dataset <- mtcars %>% select(disp,mpg)
head(dataset)
rawplot <- ggplot(dataset,mapping=aes(disp,mpg))+geom_point()
rawplot
model <- mpg ~ disp
fit <- lm(model, data = mtcars)
coeff <- coefficients(fit);intercept <- coeff[1];slope <- coeff[2]
roundIntercept <-round(coeff[1],1); roundSlope <- round(coeff[2],3)
eq <-  str_c("mpg = ",roundIntercept," + ", roundSlope, "*disp ")
print(eq)
rawplot + geom_abline(intercept = intercept, slope =slope,colour="red")
w <- c(0,0) #initial coefficients
maxdisp <- max(dataset$disp)
n <- nrow(dataset)
x1 <- rep(1,n)
x2 <- dataset$disp/maxdisp # We need to scale the data for numerical stability
y <- dataset$mpg
epsilon <- function(w){w[1]*x1+w[2]*x2-y}
lossfunction <- function(w){sum(epsilon(w)^2)/n}
partialLossW1 <- function(w){2*dot(x1,epsilon(w))/n}  #dot() from library geometry
partialLossW2 <- function(w){2*dot(x2,epsilon(w))/n}  #dot() from library geometry
stepsize <- 0.1 # I had to search for a value that resulted in good convergence
history <- NULL
for (epoch in 1:1000){
w <- w-stepsize*c(partialLossW1(w),partialLossW2(w))
loss <- lossfunction(w)
historyrecord <- as.data.frame(
list(epoch=epoch,w1=w[1],w2=w[2],partialW1=partialLossW1(w),
partialW2=partialLossW2(w),loss=loss))
if (epoch==1) history <- historyrecord else history <- rbind(history,historyrecord)
}
w[2] <- w[2]/maxdisp # unscale the coefficients
print(str_c("The intercept is ",round(w[1],3)," and the slope is ",round(w[2],3)))
w <- c(0,0) #initial coefficients
maxdisp <- max(dataset$disp)
n <- nrow(dataset)
x1 <- rep(1,n)
x2 <- dataset$disp/maxdisp # We need to scale the data for numerical stability
y <- dataset$mpg
epsilon <- function(w){w[1]*x1+w[2]*x2-y}
lossfunction <- function(w){sum(epsilon(w)^2)/n}
partialLossW1 <- function(w){2*dot(x1,epsilon(w))/n}  #dot() from library geometry
partialLossW2 <- function(w){2*dot(x2,epsilon(w))/n}  #dot() from library geometry
stepsize <- 0.1 # I had to search for a value that resulted in good convergence
history <- NULL
for (epoch in 1:1000){
w <- w-stepsize*c(partialLossW1(w),partialLossW2(w))
loss <- lossfunction(w)
historyrecord <- as.data.frame(
list(epoch=epoch,w1=w[1],w2=w[2],partialW1=partialLossW1(w),
partialW2=partialLossW2(w),loss=loss))
if (epoch==1) history <- historyrecord else history <- rbind(history,historyrecord)
}
w[2] <- w[2]/maxdisp # unscale the coefficients
print(str_c("The intercept is ",round(w[1],3)," and the slope is ",round(w[2],3)))
head(history)
tail(history)
ggplot(history,mapping=aes(epoch,loss))+geom_point()
knitr::include_graphics("figures and data/GenericNetwork.png")
#knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(stringr)
library(geometry) #dot() function for dotproduct
library(keras)
library(deepviz)  # not essential but nice to show structure
use_condaenv("tf-2.6")
dataset <- mtcars %>% select(disp,mpg)
head(dataset)
rawplot <- ggplot(dataset,mapping=aes(disp,mpg))+geom_point()
rawplot
model <- mpg ~ disp
fit <- lm(model, data = mtcars)
coeff <- coefficients(fit);intercept <- coeff[1];slope <- coeff[2]
roundIntercept <-round(coeff[1],1); roundSlope <- round(coeff[2],3)
eq <-  str_c("mpg = ",roundIntercept," + ", roundSlope, "*disp ")
print(eq)
rawplot + geom_abline(intercept = intercept, slope =slope,colour="red")
w <- c(0,0) #initial coefficients
maxdisp <- max(dataset$disp)
n <- nrow(dataset)
x1 <- rep(1,n)
x2 <- dataset$disp/maxdisp # We need to scale the data for numerical stability
y <- dataset$mpg
epsilon <- function(w){w[1]*x1+w[2]*x2-y}
lossfunction <- function(w){sum(epsilon(w)^2)/n}
partialLossW1 <- function(w){2*dot(x1,epsilon(w))/n}  #dot() from library geometry
partialLossW2 <- function(w){2*dot(x2,epsilon(w))/n}  #dot() from library geometry
stepsize <- 0.1 # I had to search for a value that resulted in good convergence
history <- NULL
for (epoch in 1:1000){
w <- w-stepsize*c(partialLossW1(w),partialLossW2(w))
loss <- lossfunction(w)
historyrecord <- as.data.frame(
list(epoch=epoch,w1=w[1],w2=w[2],partialW1=partialLossW1(w),
partialW2=partialLossW2(w),loss=loss))
if (epoch==1) history <- historyrecord else history <- rbind(history,historyrecord)
}
w[2] <- w[2]/maxdisp # unscale the coefficients
print(str_c("The intercept is ",round(w[1],3)," and the slope is ",round(w[2],3)))
head(history)
tail(history)
ggplot(history,mapping=aes(epoch,loss))+geom_point()
#knitr::include_graphics("figures and data/GenericNetwork.png")
#knitr::include_graphics("figures and data/GenericNode.png")
#knitr::include_graphics("figures and data/LinearRegressionNode.png")
#knitr::include_graphics("figures and data/RegressionNetwork.png")
library(keras)
maxdisp <- max(dataset$disp)
x_data <- dataset$disp/maxdisp # This scaling is for numerical stability
y_data <- dataset$mpg
#Create the model
model  <-  keras_model_sequential() %>%
layer_dense(units=1, activation = 'linear',
input_shape=1,name="RegressionLayer")
#Display the model
summary(model)
#knitr::include_graphics("figures and data/DenseLayer.png")
library(deepviz)
plot_model(model)
#Compile the model
# Use mean squared error (mse) for the loss function
#  and stochastic gradient descent (SGD) for the optimizer
compile(model,loss='mse', optimizer="SGD") # This makes changes directly to model
# Learn
ANNhistory <- fit(model,x_data,y_data,epochs=6000,verbose=0)
#AAN Loss Function History
ANNLossHistory <- as.data.frame(list(epochs=1:ANNhistory$params$epochs,
loss=ANNhistory$metrics$loss))
ggplot(ANNLossHistory,mapping=aes(x=epochs,y=loss))+geom_point()
print(str_c("mpg = ",roundIntercept," + ", roundSlope, "*disp "))
weights <- unlist(get_weights(model))
ANNBias <- round(weights[2],3)
ANNFeatureWeight <- round(weights[1]/maxdisp,3)
writeLines(c(str_c("The ANN bias weight is ",ANNBias),
str_c("The ANN feature weight is ",ANNFeatureWeight)))
# Predict (compute) the output
y_predicted <- predict(model,x_data)
newdataset <- dataset
newdataset$y_predicted <- y_predicted
# Display the result
rawplot +geom_point(mapping=aes(x=disp,y=y_predicted),data=newdataset,colour="red")
# Calculate the optimal order quantity
revenue_per_unit <- 19.99
cost_per_unit <- 10
mean_demand <- 10000
sd_demand <- 5000
optimal_order_quantity <- qnorm((revenue_per_unit - cost_per_unit) / revenue_per_unit) * sd_demand + mean_demand
optimal_order_quantity <- round(optimal_order_quantity)
cat("Optimal order quantity:", optimal_order_quantity, "\n")
setwd("/Users/robinyeo.hood/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/Desktop/Subjects/Uni/Term 6/Simulation Modelling and Analysis/Project/SMA-Project/Team10")
setwd("~/Library/CloudStorage/OneDrive-SingaporeUniversityofTechnologyandDesign/Desktop/Subjects/Uni/Term 6/Simulation Modelling and Analysis/Project/SMA-Project/Team10/Output Analysis")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(zoo)
library(tidyverse)
sim_data3 <- read.csv("data_3_rides.csv")
